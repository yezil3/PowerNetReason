import numpy as np
import pandas as pd
import json
import math
from pathlib import Path
import re
import os, json, re, datetime as dt
from pathlib import Path
from typing import Any, Union


def save_json(a: Union[str, Path, Any], b: Union[str, Path, Any]):
    """
    Save a Python object as pretty JSON. Creates parent dirs if needed.
    Accepts arguments in either order: (obj, path) or (path, obj).
    """
    # detect which arg is path
    if isinstance(a, (str, Path)) and not isinstance(b, (str, Path)):
        path, obj = Path(a), b
    elif isinstance(b, (str, Path)) and not isinstance(a, (str, Path)):
        path, obj = Path(b), a
    else:
        # fallback: å°½é‡æŠŠ a å½“æˆ path
        path, obj = Path(a), b

    path.parent.mkdir(parents=True, exist_ok=True)
    with path.open("w", encoding="utf-8") as f:
        json.dump(obj, f, ensure_ascii=False, indent=2)


def load_json(path):
    """
    Load JSON to Python object.
    """
    path = Path(path)
    with path.open("r", encoding="utf-8") as f:
        return json.load(f)


def save_text(text, path):
    """
    Save plain text. Creates parent dirs if needed.
    """
    path = Path(path)
    path.parent.mkdir(parents=True, exist_ok=True)
    with path.open("w", encoding="utf-8") as f:
        f.write(text)


def robust_z(x: np.ndarray):
    med = np.nanmedian(x)
    mad = np.nanmedian(np.abs(x - med)) + 1e-9
    return (x - med) / (1.4826 * mad)


def _to_1d_float(arr):
    """ç¡®ä¿æ˜¯ä¸€ç»´ float æ•°ç»„"""
    a = np.asarray(arr, dtype="float64")
    return a.ravel()


def pick_earliest_above_thresh(zone, z_thresh):
    pool = [
        c
        for c in by_zone.get(zone, [])
        if c["zscore"] >= z_thresh and not c["context_only"]
    ]
    if not pool:
        return None
    pool.sort(key=lambda x: (x["time"]))  # å­—ç¬¦ä¸²å¯ç›´æ¥æ¯”è¾ƒæˆ–è½¬ datetime
    return pool[0]


def _iso_time(t):
    """æŠŠ pandas/np æ—¶é—´å®‰å…¨è½¬æˆ ISO å­—ç¬¦ä¸²"""
    try:
        return pd.Timestamp(t).isoformat(sep=" ")
    except Exception:
        return str(t)


def robust_zscore(df: pd.DataFrame) -> pd.DataFrame:
    """Return robust z-score per column as a DataFrame with same index/columns."""
    med = df.median(axis=0, skipna=True)
    mad = (df - med).abs().median(axis=0, skipna=True)
    eps = 1e-9
    z = (df - med) / (1.4826 * (mad + eps))
    return z  # DataFrame, same index/columns as df


def build_zone_lines_from_z(
    df,
    delta_df,
    zones,
    z_thresh: float = 6.0,
    topk: int = 3,
    prefer_earliest: bool = True,
    use: str = "delta",  # "delta" æˆ– "raw"
):
    """
    ç”Ÿæˆ:
      1) zone_lines: ä¾› LLM è¯»çš„å­—ç¬¦ä¸²è¡Œ
      2) picks: [(zone_id, time_idx, z_at, delta_at)]
      3) zscore_df: å„åˆ—çš„ robust zscore DataFrame
      4) candidates: ç»“æ„åŒ–å€™é€‰åˆ—è¡¨(å¸¦ idã€è§„èŒƒåŒ–å­—æ®µ)ï¼Œä¾¿äºåç»­è°ƒè¯•/å¯è§†åŒ–/é‡æ–°æ‰“åˆ†
    """
    base = delta_df if use == "delta" else df
    zscore_df = robust_zscore(base)
    times = base.index

    zone_lines = []
    picks = []
    candidates = []  # â† æ–°å¢

    def _fmt_time(t):
        return t.strftime("%Y-%m-%d %H:%M:%S") if hasattr(t, "strftime") else str(t)

    for zi, cols in enumerate(zones, start=1):
        cols_in = [c for c in cols if c in zscore_df.columns]
        if not cols_in:
            zone_lines.append(f"Zone {zi}: none")
            continue

        z_block = zscore_df[cols_in].to_numpy(dtype=float)
        z_zone = np.nanmax(np.abs(z_block), axis=1)

        if delta_df is not None:
            d_block = delta_df[cols_in].to_numpy(dtype=float)
            delta_zone = np.nanmax(np.abs(d_block), axis=1)
        else:
            delta_zone = np.zeros_like(z_zone)

        pass_idx = np.where(z_zone >= float(z_thresh))[0]
        if pass_idx.size == 0:
            zone_lines.append(f"Zone {zi}: none")
            continue

        if prefer_earliest:
            j0 = int(pass_idx.min())
            chosen = [j0]
            remain = [j for j in pass_idx if j != j0]
            remain.sort(key=lambda j: (-abs(z_zone[j]), j))
            chosen.extend(remain[: max(0, topk - 1)])
        else:
            chosen = list(pass_idx)
            chosen.sort(key=lambda j: (-abs(z_zone[j]), j))
            chosen = chosen[:topk]

        chosen.sort()  # å±•ç¤ºæ—¶æŒ‰æ—¶é—´å‡åº
        for j in chosen:
            t = times[j]
            t_str = _fmt_time(t)
            z_at = float(z_zone[j])
            d_at = float(delta_zone[j])

            # â‘  æ–‡æœ¬è¡Œï¼ˆç»™ LLMï¼‰
            line = (
                f"Zone {zi}: {list(cols_in)}:  "
                f"t = {t_str}  ->  zscore = {z_at:.2f}  Î” = {d_at:.2f}%"
            )
            zone_lines.append(line)

            # â‘¡ è°ƒè¯•ç®€è¡¨
            picks.append((zi, j, z_at, d_at))
            z_at_t = zscore_df.loc[t_str, cols_in].abs()  # åªçœ‹è¿™ä¸ª zone çš„åˆ—
            bus_name = z_at_t.idxmax()  # åˆ—åï¼Œä¾‹å¦‚ "Appliance3"
            bus_idx = int(df.columns.get_loc(bus_name))  # åœ¨ df.columns ä¸­çš„æ•´æ•°ä½ç½®
            # â‘¢ ç»“æ„åŒ–å€™é€‰ï¼ˆç»™ä½ åšæ›´ç²¾ç»†çš„é€‰æ‹©æˆ–å¯è§†åŒ–ï¼‰
            candidates.append(
                {
                    "id": f"Z{zi}-{t_str.replace(' ', 'T')}",
                    "zone_id": int(zi),
                    # æ–°å¢ï¼šå…·ä½“è§¦å‘è¯¥å€™é€‰çš„é€šé“
                    "bus_name": bus_name,  # ä¾‹å¦‚ "Appliance3"
                    "bus_idx": bus_idx,  # ä¾‹å¦‚ 7ï¼ˆ0-basedï¼‰
                    # ä¿ç•™ä½ åŸæœ‰çš„å­—æ®µ
                    "buses": list(cols_in),
                    "time_iso": t_str,
                    "zscore": float(
                        z_at
                    ),  # ä½ åŸæ¥è®¡ç®—çš„å€™é€‰ zï¼ˆå¯ç”¨ z_at_t.max() ä¹Ÿè¡Œï¼‰
                    "delta_pct": float(d_at),  # å·²æ˜¯æ•°å€¼ï¼Œå±•ç¤ºæ—¶å†åŠ  %
                    "source": use,
                    "passed": True,
                    "threshold": float(z_thresh),
                }
            )

    return zone_lines, picks, zscore_df, candidates


def compare_llm_vs_gt(llm_json: dict, gt: dict, time_tolerance_steps: int = 1):
    """è¿”å›ä¸‰ä¸ªå¸ƒå°”ï¼šzone/bus/time æ˜¯å¦åŒ¹é…"""

    def norm_zone(x):
        try:
            return int(x) if x is not None else None
        except:
            return None

    def norm_bus(x):
        try:
            return int(x) if x is not None else None
        except:
            return None

    z_ok = norm_zone(llm_json.get("initial_zone")) == norm_zone(gt.get("initial_zone"))
    b_ok = norm_bus(llm_json.get("initial_bus")) == norm_bus(gt.get("initial_bus"))

    # æ—¶é—´ï¼šå…è®¸ Â±k æ­¥çš„å®¹å·®ï¼ˆæ—¢æ”¯æŒæ•´æ•°æ­¥ï¼Œä¹Ÿæ”¯æŒæ—¶é—´æˆ³å­—ç¬¦ä¸²ï¼‰
    lt, gt_t = llm_json.get("initial_time"), gt.get("initial_time")
    try:
        lt_ts = pd.to_datetime(str(lt))
        gt_ts = pd.to_datetime(str(gt_t))
        # æ¨æ–­ä¸€ä¸ªæ—¶é—´æ­¥é•¿
        step = None
        # è¿™é‡Œä½¿ç”¨ delta.index çš„é¢‘ç‡å¹¶ä¸æ€»èƒ½æ‹¿åˆ°ï¼›é€€åŒ–åˆ° 1 æ­¥
        step_ok = abs((lt_ts - gt_ts)) <= pd.Timedelta(minutes=1e9)  # å ä½ï¼Œä¸‹é¢æ›¿æ¢
        # å®é™…å®¹å·®ï¼šç›´æ¥æ¯”è¾ƒâ€œæ˜¯å¦ç›¸ç­‰â€æˆ–å­—ç¬¦ä¸²æ˜¯å¦ä¸€è‡´ï¼›å†é€€åŒ–ä¸ºè¿‘ä¼¼æ¯”è¾ƒ
        t_ok = (str(lt) == str(gt_t)) or (
            abs((lt_ts - gt_ts)) <= pd.Timedelta(seconds=60 * time_tolerance_steps)
        )
    except Exception:
        t_ok = str(lt) == str(gt_t)

    return z_ok, b_ok, t_ok


def _ensure_dir(p):
    Path(p).mkdir(parents=True, exist_ok=True)


# ===== Debug helpers =====
DEBUG = True  # ä¸€é”®å¼€å…³


def debug_dump(save_dir, name, content, is_json=False):
    if not DEBUG:
        return
    _ensure_dir(save_dir)
    p = Path(save_dir) / name
    if is_json:
        with open(p, "w", encoding="utf-8") as f:
            json.dump(content, f, indent=2, ensure_ascii=False)
    else:
        with open(p, "w", encoding="utf-8") as f:
            f.write(content)


def coerce_llm_json(raw_text):
    """å°½é‡ä» LLM æ–‡æœ¬ä¸­æŠ½å–ç¬¬ä¸€ä¸ª {...} JSONï¼›å¹¶åš key è§„èŒƒåŒ–ã€ç¼ºçœå¡«å……ã€‚"""
    # æŠ“ç¬¬ä¸€ä¸ªå¤§æ‹¬å· JSON
    m = re.search(r"\{.*\}", raw_text, re.S)
    js = {}
    if m:
        try:
            js = json.loads(m.group(0))
        except Exception:
            js = {}
    # å®½æ¾åœ°å…¼å®¹å­—æ®µå
    if "initial_anomaly_source" in js and isinstance(
        js["initial_anomaly_source"], dict
    ):
        src = js["initial_anomaly_source"]
        js = {
            "initial_zone": src.get("zone"),
            "initial_bus": src.get("bus_id") or src.get("bus"),
            "initial_time": src.get("time"),
            "propagation": js.get("propagation"),
            "root_cause": js.get("root_cause"),
            "recommendation": js.get("recommendation", []),
        }
    # ä¿è¯ä¸‰ä¸ªå…³é”®é”®å­˜åœ¨
    for k in ["initial_zone", "initial_bus", "initial_time"]:
        js.setdefault(k, None)
    return js


def compress_ranges(numbers):
    numbers = sorted(set(numbers))
    ranges = []
    start = end = None
    for n in numbers:
        if start is None:
            start = end = n
        elif n == end + 1:
            end = n
        else:
            ranges.append((start, end))
            start = end = n
    if start is not None:
        ranges.append((start, end))
    return ", ".join(f"{s}" if s == e else f"{s}â€“{e}" for s, e in ranges)


def robust_z_s(x):
    x = np.asarray(x, dtype=float)
    med = np.nanmedian(x)
    mad = np.nanmedian(np.abs(x - med))
    mad = mad if mad > 1e-12 else 1e-12
    return 0.6745 * (x - med) / mad


# utils.py
import json
from datetime import datetime


def pick_initial_from_candidates(path, z_thresh=6.0):
    import json, datetime as dt

    with open(path, "r") as f:
        C = json.load(f)

    # è¿‡æ»¤ï¼šé€šè¿‡é˜ˆå€¼
    C = [
        c
        for c in C
        if c.get("passed")
        and float(c.get("threshold", z_thresh)) <= float(c.get("zscore", 0))
    ]

    # è§„åˆ™ï¼šé€‰ â€œ**å…¨ä½“å€™é€‰ä¸­** æ—¶é—´æœ€æ—© çš„é‚£æ¡â€ï¼›è‹¥åŒä¸€æ—¶é—´å¤šæ¡ï¼Œåˆ™é€‰ zone_id æœ€å°
    C.sort(key=lambda c: (dt.datetime.fromisoformat(c["time_iso"]), int(c["zone_id"])))

    if not C:
        return {
            "initial_zone": None,
            "initial_time": None,
            "zscore": None,
            "delta_pct": None,
        }

    c = C[0]
    return {
        "initial_zone": int(c["zone_id"]),
        "initial_time": c["time_iso"],  # â† åªæ”¾çº¯æ—¶é—´
        "zscore": float(c.get("zscore", 0.0)),
        "delta_pct": float(c.get("delta_pct", 0.0)),
    }


def detect_time_column(df: pd.DataFrame) -> str:
    """Pick a datetime-like column (case-insensitive heuristic)."""
    candidates = [c for c in df.columns if re.search(r"time|date", str(c), re.I)]
    for c in candidates + list(df.columns):
        try:
            pd.to_datetime(df[c], errors="raise")
            return c
        except Exception:
            continue
    # fallback: create synthetic time index
    df["_idx_time_"] = np.arange(len(df))
    return "_idx_time_"


def load_clean_csv(path: str, resample: str | None) -> pd.DataFrame:
    df = pd.read_csv(path)
    tcol = detect_time_column(df)
    # Parse time
    if tcol != "_idx_time_":
        df[tcol] = pd.to_datetime(df[tcol], errors="coerce")
        df = df.dropna(subset=[tcol]).sort_values(tcol)
        df = df.set_index(tcol)
        if resample:
            # numeric-only resample with mean
            num = df.select_dtypes(include=[np.number])
            df = num.resample(resample).mean().dropna(how="all")
    else:
        # simple integer index
        df = df.set_index(tcol)

    # Keep only numeric columns as "buses"
    buses = df.select_dtypes(include=[np.number]).copy()
    # Remove constant columns to avoid NaN percent changes
    const_cols = [c for c in buses.columns if buses[c].nunique(dropna=True) <= 1]
    buses = buses.drop(columns=const_cols, errors="ignore")
    return buses


def _times_as_str(index_like):
    """æŠŠ DatetimeIndex/ä¸€èˆ¬ç´¢å¼•ç»Ÿä¸€æˆå­—ç¬¦ä¸²æ•°ç»„ï¼Œä¾¿äºå®‰å…¨ç´¢å¼•ä¸å±•ç¤ºã€‚"""
    if isinstance(index_like, pd.DatetimeIndex):
        return index_like.strftime("%Y-%m-%d %H:%M:%S").to_numpy()
    # å…¶ä»–ç±»å‹ç›´æ¥ str åŒ–
    return np.array([str(x) for x in index_like])


def compute_delta(buses: pd.DataFrame, mode: str = "pct") -> pd.DataFrame:
    """
    Return absolute delta per step:
      - pct: abs(pct_change)*100  (percent)
      - abs: abs(diff)
      - z:   abs(zscore of diff)
    """
    if mode == "pct":
        d = buses.pct_change().abs() * 100.0
    elif mode == "abs":
        d = buses.diff().abs()
    elif mode == "z":
        diff = buses.diff()
        mu = diff.mean()
        sd = diff.std().replace(0, np.nan)
        d = ((diff - mu) / sd).abs()
    else:
        raise ValueError("mode must be one of {'pct','abs','z'}")
    d = d.replace([np.inf, -np.inf], np.nan).fillna(0.0)
    return d


def split_zones(bus_names: list[str], num_zones: int) -> list[list[str]]:
    """Evenly split bus list into zones."""
    n = len(bus_names)
    num_zones = max(1, min(num_zones, n))
    k = math.ceil(n / num_zones)
    zones = [bus_names[i : i + k] for i in range(0, n, k)]
    return zones


def top_event_per_zone(delta: pd.DataFrame, zone_buses: list[str]):
    """Find (bus, time_index, value) with the largest Î” within this zone."""
    sub = delta[zone_buses]
    # locate max absolute value
    idx = np.unravel_index(np.nanargmax(sub.values), sub.shape)
    t_idx = sub.index[idx[0]]
    bus = sub.columns[idx[1]]
    val = sub.values[idx]
    return bus, t_idx, float(val)


def zone_prompt_text(bus, t, val, unit_label: str) -> str:
    # Show time as index if not datetime; if datetime, show integer step and iso time
    if isinstance(t, (np.datetime64, pd.Timestamp)):
        t_disp = f"{t}"
    else:
        t_disp = f"{t}"
    # Example line style used by your previous pipeline
    return f"Bus {bus}:  t = {t_disp} â†’ Î” = {val:.2f}{unit_label}"


def build_structure_prompt(net: pp.pandapowerNet) -> str:
    plant_buses = set(net.gen["bus"])
    load_buses = set(net.load["bus"])
    substation_buses = set(net.bus.index) - plant_buses - load_buses

    lines = net.line[["from_bus", "to_bus"]]
    adjacency = {b: set() for b in net.bus.index}
    for _, row in lines.iterrows():
        adjacency[row["from_bus"]].add(row["to_bus"])
        adjacency[row["to_bus"]].add(row["from_bus"])

    out = []
    for sb in sorted(substation_buses):
        neighbors = adjacency[sb]
        loads = sorted(b for b in neighbors if b in load_buses)
        plants = sorted(b for b in neighbors if b in plant_buses)

        entry = f"- Substation {sb}:"
        if loads:
            entry += f"\n    - Connected Loads: {compress_ranges(loads)}"
        if plants:
            entry += f"\n    - Connected Plants: {', '.join(map(str, plants))}"
        if not loads and not plants:
            entry += "\n    - No direct connections"
        out.append(entry)

    return "\n".join(out)


def summarize_voltage_data(sim, llm, tokenizer, delta_th=0.3):
    vm_df = sim.data_vm.rename(columns=lambda i: f"bus_{i}")
    va_df = sim.data_va.rename(columns=lambda i: f"bus_{i}")

    print("ğŸ” Summarizing Voltage Magnitude (vm_pu)...")
    vm_summary = summarize_columns_with_llm(
        vm_df, vm_df.columns.tolist(), llm, tokenizer, delta_th
    )

    print("\nğŸ” Summarizing Voltage Angle (va_degree)...")
    va_summary = summarize_columns_with_llm(
        va_df, va_df.columns.tolist(), llm, tokenizer, delta_th
    )

    return vm_summary, va_summary


def summarize_columns_with_llm(df, columns, llm, tokenizer, delta_th: float = 0.3):
    results = {}
    device = llm.model.device if hasattr(llm, "model") else llm.device

    for col in columns:
        stats = summarize_series_with_delta(df[col], delta_th)
        prompt = (
            "You are a powerâ€‘system timeâ€‘series analyst.\n"
            "Return a single concise sentence (max 25 words) describing trend and anomalies, using the JSON stats below.\n"
            f"Column: {col}\n"
            f"Stats: {json.dumps(stats)}"
        )
        inputs = tokenizer(
            prompt, return_tensors="pt", truncation=True, max_length=1024
        )
        inputs = {k: v.to(device) for k, v in inputs.items()}

        out = llm.generate(
            **inputs, max_new_tokens=64, pad_token_id=tokenizer.eos_token_id
        )
        resp = tokenizer.decode(out[0], skip_special_tokens=True).strip()
        results[col] = resp
        print(f"[{col}] {resp}")
    return results


def summarize_series_with_delta(series: pd.Series, delta_th: float = 0.3) -> dict:
    v = series.values.astype(float)
    diff = np.diff(v)
    sharp_idx = np.where(np.abs(diff) > delta_th)[0]

    slope = np.polyfit(np.arange(len(v)), v, 1)[0] if len(v) > 1 else 0.0
    if slope > 0.1:
        trend = "increasing"
    elif slope < -0.1:
        trend = "decreasing"
    else:
        trend = "stable"

    return {
        "mean": round(float(v.mean()), 3),
        "min": round(float(v.min()), 3),
        "max": round(float(v.max()), 3),
        "trend": trend,
        f"n_sharp_changes(>|{delta_th:.2f}|)": int(len(sharp_idx)),
        "sharp_idx_sample": sharp_idx[:5].tolist(),
    }


def summarize_system_level(llm, tokenizer, structure_prompt, vm_summary, va_summary):
    def clean(text):
        return text.split("ASSISTANT:")[-1].strip()

    vm_lines = "\n".join(f"{k} (vm_pu): {clean(v)}" for k, v in vm_summary.items())
    va_lines = "\n".join(f"{k} (va_deg): {clean(v)}" for k, v in va_summary.items())

    prompt = (
        "You are a powerâ€‘grid reliability expert.\n"
        "Based on the voltage behavior of each bus and the structure of the grid, "
        "identify major failures, likely root causes, and possible propagation paths.\n\n"
        f"Structure:\n{structure_prompt.strip()}\n\n"
        f"Voltage Magnitude Summary:\n{vm_lines}\n\n"
        f"Voltage Angle Summary:\n{va_lines}"
    )

    device = llm.model.device if hasattr(llm, "model") else llm.device
    inputs = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=2048)
    inputs = {k: v.to(device) for k, v in inputs.items()}

    output = llm.generate(
        **inputs, max_new_tokens=300, pad_token_id=tokenizer.eos_token_id
    )
    result = tokenizer.decode(output[0], skip_special_tokens=True).strip()

    print("\n================ SYSTEMâ€‘LEVEL SUMMARY ================\n")
    print(result)
    return result
